{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sacred-powell",
   "metadata": {},
   "source": [
    "## SPARK STREAMING AND MACHINE LEARNING WITH SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-denmark",
   "metadata": {},
   "source": [
    "### Topic Modeling on Moive Reviews with Spark ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absent-embassy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').config(\"spark.driver.memory\", \"2g\").appName('spark_ml_imdb').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "familiar-fashion",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "|Petter Mattei's \"...| positive|\n",
      "|Probably my all-t...| positive|\n",
      "|I sure would like...| positive|\n",
      "|This show was an ...| negative|\n",
      "|Encouraged by the...| negative|\n",
      "|If you like origi...| positive|\n",
      "|Phil the Alien is...| negative|\n",
      "|I saw this movie ...| negative|\n",
      "|So im not a big f...| negative|\n",
      "|The cast played S...| negative|\n",
      "|This a fantastic ...| positive|\n",
      "|Kind of drawn in ...| negative|\n",
      "|Some films just s...| positive|\n",
      "|This movie made i...| negative|\n",
      "|I remember this f...| positive|\n",
      "|An awful film! It...| negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.options(inferSchema = True, multiLine = True, escape = '\\\"').csv('IMDB_Reviews.csv.gz', header=True)\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-liver",
   "metadata": {},
   "source": [
    "First, we should clean up the review texts. Besides those special characters we have tried to remove in exercise, here we also need to remove the html tags in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriented-insert",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review='One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked  They are right  as this is exactly what happened with me   The first thing that struck me about Oz was its brutality and unflinching scenes of violence  which set in right from the word GO  Trust me  this is not a show for the faint hearted or timid  This show pulls no punches with regards to drugs  sex or violence  Its is hardcore  in the classic use of the word   It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald City  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  Em City is home to many Aryans  Muslims  gangstas  Latinos  Christians  Italians  Irish and more so scuffles  death stares  dodgy dealings and shady agreements are never far away   I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  Forget pretty pictures painted for mainstream audiences  forget charm  forget romance OZ doesn t mess around  The first episode I ever saw struck me as so nasty it was surreal  I couldn t say I was ready for it  but as I watched more  I developed a taste for Oz  and got accustomed to the high levels of graphic violence  Not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  Watching Oz  you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ', sentiment='positive')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "#remove html tags in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '<[^>]+>', ' '))\n",
    "#remove special characters and line breaks in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '([^\\s\\w_]|_)+', ' ')).withColumn('review', fn.regexp_replace(fn.col(\"review\"), '[\\n\\r]', ' '))\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-individual",
   "metadata": {},
   "source": [
    "Now let's create tokenizer to start the data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "circular-excellence",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_tok=['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ft.RegexTokenizer(inputCol='review', outputCol='review_tok', pattern='\\s+|[,.\\\"/!]')\n",
    "tokenizer.transform(reviews).select('review_tok').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-moses",
   "metadata": {},
   "source": [
    "Then remove stopwords in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "engaging-kingston",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_stop=['one', 'reviewers', 'mentioned', 'watching', '1', 'oz', 'episode', 'll', 'hooked', 'right', 'exactly', 'happened', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'wouldn', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'couldn', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', 'll', 'sold', 'nickel', 'inmates', 'll', 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', 'thats', 'get', 'touch', 'darker', 'side'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='review_stop')\n",
    "stopwords.transform(tokenizer.transform(reviews)).select('review_stop').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-washer",
   "metadata": {},
   "source": [
    "Now same as what we did in the exercise, let's create `CountVectorizer` to transform the text into term frequency vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finite-research",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_tf=SparseVector(101111, {2: 1.0, 10: 1.0, 13: 2.0, 17: 2.0, 28: 1.0, 32: 1.0, 35: 3.0, 39: 1.0, 45: 2.0, 46: 1.0, 50: 1.0, 53: 1.0, 54: 2.0, 57: 1.0, 83: 1.0, 85: 1.0, 91: 1.0, 93: 1.0, 97: 1.0, 101: 2.0, 108: 1.0, 121: 1.0, 128: 3.0, 138: 2.0, 160: 1.0, 161: 1.0, 169: 1.0, 174: 1.0, 184: 1.0, 191: 2.0, 195: 1.0, 217: 1.0, 235: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 264: 1.0, 278: 1.0, 286: 2.0, 302: 1.0, 316: 1.0, 324: 1.0, 370: 1.0, 386: 1.0, 409: 2.0, 438: 1.0, 448: 4.0, 453: 1.0, 459: 1.0, 480: 1.0, 501: 1.0, 514: 1.0, 526: 1.0, 534: 2.0, 535: 1.0, 567: 2.0, 582: 1.0, 685: 1.0, 707: 3.0, 754: 1.0, 779: 1.0, 826: 1.0, 921: 1.0, 939: 1.0, 1073: 3.0, 1092: 1.0, 1108: 1.0, 1146: 1.0, 1181: 1.0, 1210: 1.0, 1292: 1.0, 1293: 1.0, 1313: 1.0, 1348: 1.0, 1462: 1.0, 1475: 1.0, 1496: 1.0, 1619: 1.0, 1897: 1.0, 1917: 1.0, 1936: 1.0, 2019: 1.0, 2115: 1.0, 2206: 1.0, 2328: 1.0, 2345: 1.0, 2375: 1.0, 2422: 1.0, 2472: 1.0, 2591: 1.0, 2790: 1.0, 2794: 1.0, 2863: 1.0, 2899: 1.0, 2971: 6.0, 3086: 1.0, 3135: 2.0, 3205: 1.0, 3705: 1.0, 3727: 1.0, 3987: 1.0, 4073: 1.0, 4571: 1.0, 4784: 1.0, 4907: 1.0, 4938: 1.0, 5274: 1.0, 5407: 1.0, 5832: 1.0, 6756: 1.0, 6808: 2.0, 6963: 1.0, 7082: 1.0, 7174: 1.0, 7506: 1.0, 7590: 1.0, 7739: 1.0, 7832: 1.0, 8087: 1.0, 8646: 1.0, 9090: 1.0, 10091: 1.0, 11404: 1.0, 11630: 1.0, 11947: 1.0, 12353: 1.0, 14712: 1.0, 14897: 1.0, 15151: 1.0, 16645: 1.0, 19313: 1.0, 22856: 1.0, 22860: 1.0, 25261: 1.0, 32925: 1.0, 40472: 1.0, 48779: 1.0, 50884: 1.0, 52966: 1.0}))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf')\n",
    "tokenized = stopwords.transform(tokenizer.transform(reviews))\n",
    "tf.fit(tokenized).transform(tokenized).select('review_tf').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-mercy",
   "metadata": {},
   "source": [
    "Then we use the `LDA` model to do topic modeling. We create the model here with 30 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "received-spyware",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "lda = clus.LDA(k=30, optimizer='online', maxIter=10, featuresCol=tf.getOutputCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-princess",
   "metadata": {},
   "source": [
    "Now let's build the pipeline to train the topic model from the raw data. It will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identified-vatican",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[topicDistribution: vector]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#[Your Code] to build a ML pipeline to fit LDA\n",
    "pipeline = Pipeline(stages=[tokenizer,stopwords,tf, lda])\n",
    "pipeline_model = pipeline.fit(reviews)\n",
    "\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "\n",
    "topics.select('topicDistribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-identity",
   "metadata": {},
   "source": [
    "Let's see if we have properly discovered the topics. This is just the same code we display topics in the exercise - we will reuse it several times here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metropolitan-crest",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "movie\n",
      "really\n",
      "great\n",
      "time\n",
      "well\n",
      "best\n",
      "much\n",
      "story\n",
      "see\n",
      "made\n",
      "watch\n",
      "even\n",
      "get\n",
      "movies\n",
      "people\n",
      "two\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "good\n",
      "one\n",
      "story\n",
      "really\n",
      "like\n",
      "time\n",
      "well\n",
      "much\n",
      "even\n",
      "acting\n",
      "love\n",
      "see\n",
      "great\n",
      "get\n",
      "first\n",
      "way\n",
      "m\n",
      "watch\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "trivialboring\n",
      "well\n",
      "great\n",
      "best\n",
      "like\n",
      "much\n",
      "really\n",
      "also\n",
      "time\n",
      "better\n",
      "two\n",
      "seki\n",
      "first\n",
      "director\n",
      "characters\n",
      "see\n",
      "man\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "film\n",
      "like\n",
      "plot\n",
      "movie\n",
      "know\n",
      "one\n",
      "see\n",
      "ve\n",
      "well\n",
      "really\n",
      "m\n",
      "characters\n",
      "real\n",
      "character\n",
      "ending\n",
      "say\n",
      "seen\n",
      "even\n",
      "also\n",
      "time\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "story\n",
      "much\n",
      "good\n",
      "even\n",
      "time\n",
      "films\n",
      "life\n",
      "also\n",
      "better\n",
      "really\n",
      "see\n",
      "many\n",
      "seen\n",
      "get\n",
      "way\n",
      "love\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "bad\n",
      "time\n",
      "people\n",
      "even\n",
      "good\n",
      "well\n",
      "story\n",
      "also\n",
      "get\n",
      "best\n",
      "love\n",
      "life\n",
      "characters\n",
      "great\n",
      "sex\n",
      "doug\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "film\n",
      "one\n",
      "like\n",
      "also\n",
      "good\n",
      "movie\n",
      "first\n",
      "scene\n",
      "son\n",
      "much\n",
      "well\n",
      "story\n",
      "two\n",
      "another\n",
      "way\n",
      "version\n",
      "old\n",
      "danni\n",
      "made\n",
      "time\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "time\n",
      "people\n",
      "good\n",
      "well\n",
      "also\n",
      "story\n",
      "really\n",
      "see\n",
      "think\n",
      "much\n",
      "first\n",
      "even\n",
      "bad\n",
      "little\n",
      "great\n",
      "character\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "good\n",
      "way\n",
      "time\n",
      "see\n",
      "world\n",
      "get\n",
      "films\n",
      "even\n",
      "though\n",
      "10\n",
      "take\n",
      "made\n",
      "history\n",
      "plot\n",
      "also\n",
      "characters\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "even\n",
      "good\n",
      "well\n",
      "see\n",
      "way\n",
      "time\n",
      "much\n",
      "get\n",
      "also\n",
      "make\n",
      "really\n",
      "bad\n",
      "character\n",
      "story\n",
      "people\n",
      "first\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "bad\n",
      "get\n",
      "never\n",
      "even\n",
      "show\n",
      "time\n",
      "good\n",
      "like\n",
      "much\n",
      "best\n",
      "barney\n",
      "thing\n",
      "kids\n",
      "story\n",
      "character\n",
      "see\n",
      "really\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "see\n",
      "also\n",
      "well\n",
      "really\n",
      "get\n",
      "even\n",
      "films\n",
      "never\n",
      "movies\n",
      "time\n",
      "bad\n",
      "think\n",
      "story\n",
      "life\n",
      "love\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "one\n",
      "movie\n",
      "scene\n",
      "first\n",
      "show\n",
      "love\n",
      "film\n",
      "like\n",
      "even\n",
      "made\n",
      "much\n",
      "well\n",
      "also\n",
      "woman\n",
      "ve\n",
      "make\n",
      "really\n",
      "buddy\n",
      "get\n",
      "see\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "good\n",
      "well\n",
      "first\n",
      "even\n",
      "best\n",
      "think\n",
      "way\n",
      "feinstone\n",
      "really\n",
      "great\n",
      "time\n",
      "end\n",
      "story\n",
      "horror\n",
      "man\n",
      "acting\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "film\n",
      "like\n",
      "movie\n",
      "great\n",
      "see\n",
      "story\n",
      "really\n",
      "people\n",
      "think\n",
      "much\n",
      "one\n",
      "dvd\n",
      "good\n",
      "love\n",
      "seen\n",
      "films\n",
      "character\n",
      "years\n",
      "make\n",
      "life\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "film\n",
      "one\n",
      "time\n",
      "story\n",
      "movie\n",
      "like\n",
      "even\n",
      "character\n",
      "back\n",
      "good\n",
      "also\n",
      "see\n",
      "think\n",
      "really\n",
      "well\n",
      "two\n",
      "life\n",
      "many\n",
      "never\n",
      "films\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "edmund\n",
      "even\n",
      "good\n",
      "story\n",
      "also\n",
      "abigail\n",
      "people\n",
      "like\n",
      "first\n",
      "two\n",
      "well\n",
      "come\n",
      "times\n",
      "bad\n",
      "great\n",
      "game\n",
      "father\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "like\n",
      "movie\n",
      "snakes\n",
      "one\n",
      "snake\n",
      "train\n",
      "langdon\n",
      "characters\n",
      "also\n",
      "good\n",
      "harry\n",
      "child\n",
      "woman\n",
      "time\n",
      "enough\n",
      "novel\n",
      "mrs\n",
      "soon\n",
      "afternoon\n",
      "big\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "see\n",
      "time\n",
      "good\n",
      "two\n",
      "people\n",
      "made\n",
      "story\n",
      "much\n",
      "watch\n",
      "get\n",
      "well\n",
      "way\n",
      "seen\n",
      "first\n",
      "life\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "one\n",
      "good\n",
      "show\n",
      "even\n",
      "great\n",
      "life\n",
      "time\n",
      "story\n",
      "best\n",
      "well\n",
      "people\n",
      "man\n",
      "never\n",
      "make\n",
      "acting\n",
      "custer\n",
      "ve\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "one\n",
      "film\n",
      "movie\n",
      "like\n",
      "way\n",
      "good\n",
      "get\n",
      "time\n",
      "well\n",
      "show\n",
      "also\n",
      "make\n",
      "first\n",
      "say\n",
      "really\n",
      "see\n",
      "people\n",
      "look\n",
      "still\n",
      "even\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "time\n",
      "even\n",
      "story\n",
      "really\n",
      "see\n",
      "well\n",
      "much\n",
      "great\n",
      "bad\n",
      "people\n",
      "get\n",
      "also\n",
      "first\n",
      "movies\n",
      "made\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "see\n",
      "good\n",
      "time\n",
      "really\n",
      "get\n",
      "even\n",
      "films\n",
      "way\n",
      "much\n",
      "well\n",
      "bad\n",
      "man\n",
      "also\n",
      "plot\n",
      "first\n",
      "two\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "family\n",
      "well\n",
      "good\n",
      "really\n",
      "time\n",
      "characters\n",
      "like\n",
      "way\n",
      "make\n",
      "gram\n",
      "great\n",
      "life\n",
      "story\n",
      "much\n",
      "also\n",
      "even\n",
      "movies\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "people\n",
      "one\n",
      "film\n",
      "good\n",
      "time\n",
      "made\n",
      "first\n",
      "show\n",
      "really\n",
      "much\n",
      "even\n",
      "see\n",
      "two\n",
      "way\n",
      "movies\n",
      "end\n",
      "character\n",
      "guy\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "scene\n",
      "story\n",
      "like\n",
      "well\n",
      "really\n",
      "even\n",
      "time\n",
      "character\n",
      "also\n",
      "watch\n",
      "two\n",
      "see\n",
      "never\n",
      "thing\n",
      "something\n",
      "get\n",
      "go\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "movie\n",
      "show\n",
      "time\n",
      "one\n",
      "film\n",
      "like\n",
      "also\n",
      "well\n",
      "ever\n",
      "two\n",
      "characters\n",
      "every\n",
      "good\n",
      "even\n",
      "go\n",
      "family\n",
      "people\n",
      "great\n",
      "best\n",
      "first\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "first\n",
      "film\n",
      "see\n",
      "like\n",
      "show\n",
      "story\n",
      "time\n",
      "m\n",
      "good\n",
      "really\n",
      "people\n",
      "make\n",
      "movies\n",
      "get\n",
      "many\n",
      "watch\n",
      "bad\n",
      "way\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "film\n",
      "one\n",
      "dixon\n",
      "well\n",
      "like\n",
      "movie\n",
      "even\n",
      "movies\n",
      "director\n",
      "two\n",
      "italian\n",
      "paine\n",
      "nr\n",
      "blood\n",
      "made\n",
      "films\n",
      "great\n",
      "howard\n",
      "d\n",
      "shark\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "time\n",
      "like\n",
      "even\n",
      "story\n",
      "good\n",
      "movies\n",
      "great\n",
      "get\n",
      "characters\n",
      "films\n",
      "make\n",
      "seen\n",
      "character\n",
      "people\n",
      "man\n",
      "bad\n",
      "plot\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-genealogy",
   "metadata": {},
   "source": [
    "How do you think about the topics? Do they make sense? If you think the topics we get from the movie reviews should be better, let's continue to see what we can do to make them better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-paste",
   "metadata": {},
   "source": [
    "One possible reason is that we have many words that do not show up frequently. That is, they are very specific words to certain movies but don't occur across reviews. Such words are not very meaningful and they do not represent common themes in those reviews. So here we limit the frequency of words to at least 5 and run LDA with pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "informative-share",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter the countvectorizer\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf', minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amino-pathology",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[topicDistribution: vector]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,stopwords,tf, lda])\n",
    "pipeline_model = pipeline.fit(reviews)\n",
    "\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "\n",
    "topics.select('topicDistribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "union-corps",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "ponyo\n",
      "good\n",
      "time\n",
      "much\n",
      "well\n",
      "even\n",
      "really\n",
      "great\n",
      "story\n",
      "people\n",
      "also\n",
      "two\n",
      "see\n",
      "love\n",
      "best\n",
      "man\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "see\n",
      "time\n",
      "good\n",
      "story\n",
      "like\n",
      "really\n",
      "even\n",
      "much\n",
      "well\n",
      "character\n",
      "watch\n",
      "great\n",
      "think\n",
      "seen\n",
      "films\n",
      "many\n",
      "director\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "one\n",
      "people\n",
      "good\n",
      "time\n",
      "film\n",
      "puerto\n",
      "even\n",
      "watch\n",
      "really\n",
      "way\n",
      "much\n",
      "life\n",
      "movies\n",
      "make\n",
      "know\n",
      "get\n",
      "m\n",
      "comedy\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "good\n",
      "even\n",
      "like\n",
      "love\n",
      "see\n",
      "really\n",
      "time\n",
      "story\n",
      "life\n",
      "acting\n",
      "also\n",
      "get\n",
      "way\n",
      "bad\n",
      "much\n",
      "films\n",
      "make\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "good\n",
      "like\n",
      "othello\n",
      "characters\n",
      "make\n",
      "get\n",
      "din\n",
      "films\n",
      "scene\n",
      "seen\n",
      "well\n",
      "story\n",
      "see\n",
      "first\n",
      "ever\n",
      "time\n",
      "made\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "movie\n",
      "lincoln\n",
      "like\n",
      "really\n",
      "people\n",
      "also\n",
      "film\n",
      "even\n",
      "good\n",
      "way\n",
      "one\n",
      "first\n",
      "well\n",
      "story\n",
      "much\n",
      "films\n",
      "character\n",
      "man\n",
      "part\n",
      "great\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "first\n",
      "well\n",
      "time\n",
      "also\n",
      "man\n",
      "years\n",
      "m\n",
      "good\n",
      "films\n",
      "like\n",
      "even\n",
      "character\n",
      "great\n",
      "love\n",
      "story\n",
      "see\n",
      "series\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "kelly\n",
      "one\n",
      "good\n",
      "story\n",
      "well\n",
      "like\n",
      "time\n",
      "great\n",
      "custer\n",
      "best\n",
      "much\n",
      "bad\n",
      "really\n",
      "way\n",
      "sinatra\n",
      "characters\n",
      "even\n",
      "get\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "show\n",
      "like\n",
      "movie\n",
      "one\n",
      "time\n",
      "life\n",
      "even\n",
      "well\n",
      "also\n",
      "family\n",
      "man\n",
      "good\n",
      "much\n",
      "first\n",
      "get\n",
      "antwone\n",
      "character\n",
      "john\n",
      "portuguese\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "one\n",
      "good\n",
      "see\n",
      "watch\n",
      "ever\n",
      "funny\n",
      "get\n",
      "really\n",
      "time\n",
      "seen\n",
      "even\n",
      "bad\n",
      "think\n",
      "never\n",
      "first\n",
      "make\n",
      "made\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "even\n",
      "way\n",
      "never\n",
      "much\n",
      "scene\n",
      "well\n",
      "really\n",
      "people\n",
      "also\n",
      "see\n",
      "good\n",
      "characters\n",
      "character\n",
      "bad\n",
      "seen\n",
      "made\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "even\n",
      "time\n",
      "story\n",
      "love\n",
      "great\n",
      "also\n",
      "know\n",
      "tv\n",
      "american\n",
      "still\n",
      "think\n",
      "good\n",
      "made\n",
      "people\n",
      "best\n",
      "man\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "really\n",
      "like\n",
      "time\n",
      "watch\n",
      "evelyn\n",
      "get\n",
      "munchies\n",
      "good\n",
      "way\n",
      "even\n",
      "bad\n",
      "two\n",
      "people\n",
      "made\n",
      "also\n",
      "lady\n",
      "well\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "bad\n",
      "show\n",
      "good\n",
      "really\n",
      "way\n",
      "films\n",
      "even\n",
      "story\n",
      "scenes\n",
      "plot\n",
      "two\n",
      "see\n",
      "make\n",
      "first\n",
      "get\n",
      "people\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "good\n",
      "bad\n",
      "well\n",
      "one\n",
      "like\n",
      "also\n",
      "really\n",
      "even\n",
      "people\n",
      "characters\n",
      "see\n",
      "movies\n",
      "story\n",
      "plot\n",
      "never\n",
      "know\n",
      "watch\n",
      "films\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "like\n",
      "people\n",
      "think\n",
      "see\n",
      "time\n",
      "good\n",
      "really\n",
      "movies\n",
      "film\n",
      "make\n",
      "even\n",
      "bad\n",
      "well\n",
      "first\n",
      "best\n",
      "much\n",
      "something\n",
      "show\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "first\n",
      "time\n",
      "well\n",
      "nothing\n",
      "scene\n",
      "get\n",
      "story\n",
      "walker\n",
      "also\n",
      "much\n",
      "scenes\n",
      "way\n",
      "re\n",
      "good\n",
      "best\n",
      "made\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "rockne\n",
      "like\n",
      "one\n",
      "time\n",
      "re\n",
      "good\n",
      "well\n",
      "carlisle\n",
      "people\n",
      "see\n",
      "reagan\n",
      "celeste\n",
      "football\n",
      "notre\n",
      "brien\n",
      "make\n",
      "dame\n",
      "say\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "time\n",
      "even\n",
      "really\n",
      "see\n",
      "story\n",
      "well\n",
      "great\n",
      "much\n",
      "get\n",
      "bad\n",
      "also\n",
      "first\n",
      "people\n",
      "movies\n",
      "made\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "really\n",
      "movie\n",
      "see\n",
      "like\n",
      "show\n",
      "film\n",
      "one\n",
      "great\n",
      "moore\n",
      "ever\n",
      "even\n",
      "time\n",
      "ve\n",
      "characters\n",
      "well\n",
      "think\n",
      "saw\n",
      "seen\n",
      "let\n",
      "way\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "film\n",
      "one\n",
      "story\n",
      "movie\n",
      "like\n",
      "way\n",
      "see\n",
      "life\n",
      "much\n",
      "even\n",
      "series\n",
      "really\n",
      "time\n",
      "think\n",
      "get\n",
      "also\n",
      "well\n",
      "films\n",
      "first\n",
      "good\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "great\n",
      "story\n",
      "even\n",
      "also\n",
      "plot\n",
      "well\n",
      "man\n",
      "film\n",
      "good\n",
      "scenes\n",
      "watch\n",
      "see\n",
      "cast\n",
      "horror\n",
      "best\n",
      "time\n",
      "two\n",
      "films\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "even\n",
      "time\n",
      "much\n",
      "good\n",
      "people\n",
      "two\n",
      "story\n",
      "war\n",
      "man\n",
      "character\n",
      "bad\n",
      "first\n",
      "also\n",
      "get\n",
      "life\n",
      "well\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "film\n",
      "hedge\n",
      "marine\n",
      "love\n",
      "first\n",
      "one\n",
      "max\n",
      "two\n",
      "people\n",
      "also\n",
      "movie\n",
      "cement\n",
      "think\n",
      "like\n",
      "really\n",
      "us\n",
      "head\n",
      "many\n",
      "morty\n",
      "story\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "film\n",
      "time\n",
      "like\n",
      "gandhi\n",
      "one\n",
      "story\n",
      "movie\n",
      "much\n",
      "barney\n",
      "great\n",
      "show\n",
      "get\n",
      "even\n",
      "never\n",
      "made\n",
      "ever\n",
      "first\n",
      "good\n",
      "loved\n",
      "new\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "time\n",
      "also\n",
      "even\n",
      "people\n",
      "well\n",
      "story\n",
      "good\n",
      "still\n",
      "two\n",
      "first\n",
      "life\n",
      "new\n",
      "man\n",
      "much\n",
      "little\n",
      "black\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "dixon\n",
      "zizek\n",
      "time\n",
      "paine\n",
      "like\n",
      "films\n",
      "scalise\n",
      "even\n",
      "people\n",
      "story\n",
      "good\n",
      "make\n",
      "watch\n",
      "characters\n",
      "little\n",
      "also\n",
      "velvet\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "good\n",
      "film\n",
      "like\n",
      "movies\n",
      "great\n",
      "time\n",
      "seen\n",
      "bad\n",
      "really\n",
      "made\n",
      "acting\n",
      "first\n",
      "well\n",
      "see\n",
      "even\n",
      "also\n",
      "never\n",
      "still\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "one\n",
      "like\n",
      "film\n",
      "really\n",
      "time\n",
      "movie\n",
      "re\n",
      "scene\n",
      "never\n",
      "though\n",
      "well\n",
      "real\n",
      "plot\n",
      "even\n",
      "way\n",
      "story\n",
      "great\n",
      "made\n",
      "around\n",
      "de\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "see\n",
      "one\n",
      "time\n",
      "plot\n",
      "even\n",
      "way\n",
      "well\n",
      "elvira\n",
      "get\n",
      "really\n",
      "old\n",
      "good\n",
      "bad\n",
      "go\n",
      "story\n",
      "characters\n",
      "people\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-semiconductor",
   "metadata": {},
   "source": [
    "It is expected that the topics are getting better but still not very satisfying. Some words may be very specific to some reviews. Also, there are lots of words shown in different topics many times; possibly they are too common so they shouldn't be that important. Let's take one more step to use TF-IDF vector rather than TF vector. To build IF-IDF, we first create TF with CountVectorizer then create IDF from TF vector. Then we run LDA model with IF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latest-making",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use tf-idf vector\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol=\"review_tf\", vocabSize=10000)\n",
    "idf = ft.IDF(inputCol=tf.getOutputCol(), outputCol=\"review_tfidf\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "strange-purchase",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[topicDistribution: vector]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,stopwords,tf,idf,lda])\n",
    "pipeline_model = pipeline.fit(reviews)\n",
    "\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "\n",
    "topics.select('topicDistribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "headed-placement",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "one\n",
      "stewart\n",
      "story\n",
      "tarzan\n",
      "western\n",
      "good\n",
      "movie\n",
      "fulci\n",
      "well\n",
      "chan\n",
      "great\n",
      "like\n",
      "jackie\n",
      "best\n",
      "man\n",
      "much\n",
      "scott\n",
      "dean\n",
      "made\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "great\n",
      "dance\n",
      "musical\n",
      "even\n",
      "better\n",
      "love\n",
      "story\n",
      "superman\n",
      "time\n",
      "see\n",
      "best\n",
      "much\n",
      "get\n",
      "make\n",
      "people\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "good\n",
      "time\n",
      "well\n",
      "film\n",
      "characters\n",
      "story\n",
      "life\n",
      "two\n",
      "much\n",
      "komodo\n",
      "best\n",
      "acting\n",
      "cobra\n",
      "really\n",
      "like\n",
      "way\n",
      "first\n",
      "donna\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "really\n",
      "well\n",
      "way\n",
      "book\n",
      "good\n",
      "people\n",
      "one\n",
      "work\n",
      "think\n",
      "also\n",
      "team\n",
      "bad\n",
      "plot\n",
      "great\n",
      "new\n",
      "lots\n",
      "acting\n",
      "another\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "film\n",
      "one\n",
      "like\n",
      "show\n",
      "movie\n",
      "people\n",
      "see\n",
      "even\n",
      "really\n",
      "first\n",
      "season\n",
      "ford\n",
      "bad\n",
      "good\n",
      "time\n",
      "well\n",
      "family\n",
      "episode\n",
      "series\n",
      "episodes\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "film\n",
      "one\n",
      "well\n",
      "good\n",
      "really\n",
      "also\n",
      "never\n",
      "movie\n",
      "horror\n",
      "much\n",
      "friends\n",
      "young\n",
      "made\n",
      "version\n",
      "great\n",
      "might\n",
      "films\n",
      "best\n",
      "give\n",
      "better\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "film\n",
      "kong\n",
      "movie\n",
      "one\n",
      "like\n",
      "even\n",
      "films\n",
      "really\n",
      "good\n",
      "real\n",
      "great\n",
      "see\n",
      "way\n",
      "lot\n",
      "malkovich\n",
      "us\n",
      "make\n",
      "time\n",
      "king\n",
      "also\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "time\n",
      "films\n",
      "even\n",
      "first\n",
      "series\n",
      "well\n",
      "batman\n",
      "seen\n",
      "story\n",
      "people\n",
      "two\n",
      "see\n",
      "good\n",
      "also\n",
      "great\n",
      "much\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "also\n",
      "one\n",
      "life\n",
      "much\n",
      "well\n",
      "still\n",
      "beethoven\n",
      "love\n",
      "cobb\n",
      "good\n",
      "surfing\n",
      "people\n",
      "give\n",
      "man\n",
      "best\n",
      "real\n",
      "time\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "emma\n",
      "austen\n",
      "movie\n",
      "chucky\n",
      "story\n",
      "fanny\n",
      "edmund\n",
      "paltrow\n",
      "good\n",
      "tilly\n",
      "like\n",
      "love\n",
      "beckinsale\n",
      "one\n",
      "see\n",
      "gwyneth\n",
      "make\n",
      "comedy\n",
      "time\n",
      "film\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "movie\n",
      "really\n",
      "custer\n",
      "film\n",
      "well\n",
      "like\n",
      "great\n",
      "sharky\n",
      "puerto\n",
      "see\n",
      "good\n",
      "costner\n",
      "even\n",
      "one\n",
      "dirk\n",
      "never\n",
      "boogie\n",
      "story\n",
      "way\n",
      "time\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "story\n",
      "like\n",
      "life\n",
      "well\n",
      "time\n",
      "also\n",
      "man\n",
      "people\n",
      "two\n",
      "even\n",
      "many\n",
      "love\n",
      "great\n",
      "way\n",
      "world\n",
      "see\n",
      "good\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "one\n",
      "good\n",
      "scooby\n",
      "people\n",
      "really\n",
      "show\n",
      "doo\n",
      "never\n",
      "characters\n",
      "way\n",
      "watch\n",
      "see\n",
      "moore\n",
      "movies\n",
      "something\n",
      "even\n",
      "lot\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "match\n",
      "like\n",
      "even\n",
      "time\n",
      "allen\n",
      "also\n",
      "made\n",
      "good\n",
      "many\n",
      "really\n",
      "think\n",
      "make\n",
      "bad\n",
      "holmes\n",
      "never\n",
      "get\n",
      "two\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "muppet\n",
      "story\n",
      "really\n",
      "good\n",
      "well\n",
      "character\n",
      "made\n",
      "bad\n",
      "way\n",
      "also\n",
      "time\n",
      "even\n",
      "movies\n",
      "cole\n",
      "see\n",
      "people\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "wagner\n",
      "rusty\n",
      "get\n",
      "story\n",
      "man\n",
      "great\n",
      "m\n",
      "time\n",
      "really\n",
      "characters\n",
      "also\n",
      "movies\n",
      "kelly\n",
      "people\n",
      "good\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "movie\n",
      "time\n",
      "film\n",
      "story\n",
      "one\n",
      "war\n",
      "much\n",
      "make\n",
      "characters\n",
      "end\n",
      "acting\n",
      "norwegian\n",
      "like\n",
      "really\n",
      "young\n",
      "movies\n",
      "character\n",
      "actual\n",
      "even\n",
      "hardy\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "film\n",
      "garbo\n",
      "party\n",
      "silent\n",
      "films\n",
      "house\n",
      "mighty\n",
      "one\n",
      "think\n",
      "director\n",
      "series\n",
      "4\n",
      "magnificently\n",
      "cinematographer\n",
      "restored\n",
      "paramount\n",
      "humor\n",
      "magnificent\n",
      "audiences\n",
      "part\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "really\n",
      "like\n",
      "vivian\n",
      "even\n",
      "good\n",
      "way\n",
      "plot\n",
      "think\n",
      "get\n",
      "book\n",
      "gabriel\n",
      "well\n",
      "see\n",
      "also\n",
      "though\n",
      "time\n",
      "thing\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "film\n",
      "one\n",
      "game\n",
      "movie\n",
      "hitchcock\n",
      "good\n",
      "much\n",
      "time\n",
      "like\n",
      "even\n",
      "see\n",
      "great\n",
      "plot\n",
      "man\n",
      "get\n",
      "train\n",
      "hardy\n",
      "story\n",
      "really\n",
      "made\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "gamera\n",
      "film\n",
      "one\n",
      "movie\n",
      "predator\n",
      "show\n",
      "good\n",
      "time\n",
      "even\n",
      "like\n",
      "scene\n",
      "years\n",
      "bad\n",
      "movies\n",
      "way\n",
      "first\n",
      "get\n",
      "never\n",
      "m\n",
      "story\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "film\n",
      "one\n",
      "story\n",
      "movie\n",
      "like\n",
      "trek\n",
      "good\n",
      "time\n",
      "also\n",
      "life\n",
      "well\n",
      "man\n",
      "much\n",
      "people\n",
      "really\n",
      "space\n",
      "us\n",
      "characters\n",
      "first\n",
      "get\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "great\n",
      "like\n",
      "story\n",
      "good\n",
      "little\n",
      "one\n",
      "even\n",
      "time\n",
      "well\n",
      "bad\n",
      "plot\n",
      "movies\n",
      "watch\n",
      "much\n",
      "made\n",
      "didn\n",
      "also\n",
      "gandhi\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "like\n",
      "film\n",
      "howard\n",
      "bakshi\n",
      "gay\n",
      "plot\n",
      "character\n",
      "love\n",
      "argento\n",
      "scenes\n",
      "work\n",
      "first\n",
      "much\n",
      "see\n",
      "best\n",
      "course\n",
      "scene\n",
      "really\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "ray\n",
      "ponyo\n",
      "film\n",
      "miyazaki\n",
      "elvis\n",
      "sosuke\n",
      "one\n",
      "movie\n",
      "charles\n",
      "foxx\n",
      "felix\n",
      "like\n",
      "jamie\n",
      "great\n",
      "oscar\n",
      "films\n",
      "first\n",
      "time\n",
      "lemmon\n",
      "even\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "pam\n",
      "bud\n",
      "sissy\n",
      "monkees\n",
      "story\n",
      "like\n",
      "well\n",
      "much\n",
      "two\n",
      "also\n",
      "see\n",
      "time\n",
      "films\n",
      "great\n",
      "still\n",
      "cowboy\n",
      "many\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "movie\n",
      "columbo\n",
      "like\n",
      "film\n",
      "one\n",
      "people\n",
      "well\n",
      "see\n",
      "bad\n",
      "evelyn\n",
      "ve\n",
      "time\n",
      "story\n",
      "make\n",
      "characters\n",
      "go\n",
      "kitty\n",
      "get\n",
      "luther\n",
      "black\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "one\n",
      "movie\n",
      "much\n",
      "like\n",
      "film\n",
      "rangers\n",
      "story\n",
      "really\n",
      "better\n",
      "way\n",
      "well\n",
      "poirot\n",
      "first\n",
      "also\n",
      "get\n",
      "people\n",
      "re\n",
      "good\n",
      "time\n",
      "two\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "zoey\n",
      "also\n",
      "like\n",
      "good\n",
      "two\n",
      "time\n",
      "well\n",
      "first\n",
      "young\n",
      "much\n",
      "character\n",
      "another\n",
      "even\n",
      "around\n",
      "story\n",
      "get\n",
      "characters\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "really\n",
      "even\n",
      "time\n",
      "see\n",
      "bad\n",
      "get\n",
      "much\n",
      "story\n",
      "well\n",
      "great\n",
      "movies\n",
      "first\n",
      "make\n",
      "made\n",
      "people\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "tf_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[4]\n",
    "vocab = tf_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-notification",
   "metadata": {},
   "source": [
    "The topics should be more reasonable now. You should believe they can still be further improved by cleaning up the text and tuning the hyperparameter.We can change the model configuration to see if you can get any further improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-regular",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Analysis with Spark ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infrared-anime",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sentiment|\n",
      "+---------+\n",
      "| positive|\n",
      "| negative|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first let's confirm the potential labels\n",
    "reviews.select('sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "asian-halifax",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+\n",
      "|              review|sentiment|sentiment_label|\n",
      "+--------------------+---------+---------------+\n",
      "|One of the other ...| positive|            1.0|\n",
      "|A wonderful littl...| positive|            1.0|\n",
      "|I thought this wa...| positive|            1.0|\n",
      "|Basically there s...| negative|            0.0|\n",
      "|Petter Mattei s  ...| positive|            1.0|\n",
      "|Probably my all t...| positive|            1.0|\n",
      "|I sure would like...| positive|            1.0|\n",
      "|This show was an ...| negative|            0.0|\n",
      "|Encouraged by the...| negative|            0.0|\n",
      "|If you like origi...| positive|            1.0|\n",
      "|Phil the Alien is...| negative|            0.0|\n",
      "|I saw this movie ...| negative|            0.0|\n",
      "|So im not a big f...| negative|            0.0|\n",
      "|The cast played S...| negative|            0.0|\n",
      "|This a fantastic ...| positive|            1.0|\n",
      "|Kind of drawn in ...| negative|            0.0|\n",
      "|Some films just s...| positive|            1.0|\n",
      "|This movie made i...| negative|            0.0|\n",
      "|I remember this f...| positive|            1.0|\n",
      "|An awful film  It...| negative|            0.0|\n",
      "+--------------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's create the binary numerical lable from postive/negative\n",
    "reviews = reviews.withColumn('sentiment_label', fn.when(fn.col('sentiment')=='positive', 1.0).otherwise(0.0))\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "social-dream",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split the training and testing set, with 80/20\n",
    "reviews_train, reviews_test = reviews.randomSplit([0.8, 0.2], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "present-tiger",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=idf.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "prescription-president",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,stopwords,tf,idf,lr])\n",
    "lr_model = pipeline.fit(reviews_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "going-shoot",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make predictions with pipeline model\n",
    "predictions = lr_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1cce7b4-5eb2-4020-a9b7-b449326ad452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review',\n",
       " 'sentiment',\n",
       " 'sentiment_label',\n",
       " 'review_tok',\n",
       " 'review_stop',\n",
       " 'review_tf',\n",
       " 'review_tfidf',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "official-freeze",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9338446403526336\n",
      "0.9273869863834877\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "#model evaluation for binary classification\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-celtic",
   "metadata": {},
   "source": [
    "The prediction performance looks acceptable. Here note that TF-IDF is a long vector (here we select top 10000 words, but still a large number), so let's try something different. As mentioned in the class, another way to model text is word embedding with the Word2Vec model. So next we create word vector and use it to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "provincial-senator",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create word2vec model\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "possible-explorer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#same logistic regression model, but take output from word2vec model\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=word2vec.getOutputCol())\n",
    "pipeline = Pipeline(stages=[tokenizer,stopwords,word2vec,lr])\n",
    "lr_model = pipeline.fit(reviews_train)\n",
    "predictions = lr_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05c9d520-e72d-4c71-93f6-8bff65a006c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336958725706269\n",
      "0.9304295574003703\n"
     ]
    }
   ],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-disposition",
   "metadata": {},
   "source": [
    "Check the prediction performance with only 100 features; word2vec model is a very useful representation of words and it reduces dimensionality significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-enclosure",
   "metadata": {},
   "source": [
    "In the end, let's try an alternative model. In classfication, Support Vector Machine (SVM) is commonly used and let's see how we can use it here. We will keep the orginal configuration of word2vec model (vector size is 100) here for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "challenging-hepatitis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#same word2vec model configuration is adopted here\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")\n",
    "#create svm with LinearSVC, with features from word2vec model outputCol\n",
    "svm = cl.LinearSVC(maxIter=10, labelCol='sentiment_label', featuresCol=word2vec.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "toxic-shepherd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build the ml pipeline and train the model; then make predictions \n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,stopwords,word2vec,svm])\n",
    "svm_model = pipeline.fit(reviews_train)\n",
    "predictions = svm_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "endangered-yacht",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933331116008816\n",
      "0.9304339792841155\n"
     ]
    }
   ],
   "source": [
    "#model evaluation, here slightly different for LinearSVC\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b514e-9e8a-4ca5-8cc0-19a06cdc2492",
   "metadata": {},
   "source": [
    "### SPARK STREAMING FOR REAL TIME PREDICTIONS\n",
    "Now we have our sentiment prediction model with acceptable predictive performance.That is, with a data stream, we will use the trained model to make real-time predictions. We will still use IMDB reviews and here we will create a simulated data stream from files in a directory, then receive the review stream and predict sentiment using spark structured streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cc09c-14e4-4af9-a1d6-d52d222fa1a3",
   "metadata": {},
   "source": [
    "We will first use `streaming_producer.ipynb` specifically for this so that we can simulate a data stream to send files incrementally into a directory and we can read stream from the same directory. To do that, we use `streaming_producer.ipynb` - it will send random reviews in the form of csv files into `review_stream` directory. Then this spark application will read review stream data from `review_stream` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "blank-officer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews_test_stream = reviews_test.select('review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "interracial-reception",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#here we save the data as csv to allow streaming_producer.ipynb to take random reviews from it\n",
    "df = reviews_test_stream.toPandas()\n",
    "df.to_csv('review_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41997310-557e-43b3-a0b3-7c8c5632e61d",
   "metadata": {},
   "source": [
    "Now we can read from the stream. For convenience, we just take the existing spark dataframe to reuse the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5614a-85f0-41d8-8ded-a970212fa84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run streaming_producer.ipynb notebook (but make sure you try to work on the code below first), then go back here to run subsequent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "assured-symbol",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read from data stream in a folder\n",
    "streaming_path = './review_stream'\n",
    "if not os.path.exists(streaming_path):\n",
    "    os.makedirs(streaming_path)\n",
    "streaming_review = spark.readStream.schema(reviews_test_stream.schema).option(\"maxFilesPerTrigger\", 1).csv(streaming_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a3e15cef-cff4-4b10-b08e-cb8867ad7337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643e54c-4e77-4259-9287-781c877a432f",
   "metadata": {},
   "source": [
    "Here from the data stream, we want to know three results. First, how many positive or negative reviews we have received in real time? Second, how many positive or negative reviews in each time window of 60 seconds? Third, we are interested in the positive and negative reviews in each time window of 60 seconds, but with sliding window for every 30 seconds. So we will do some calculations, and to capture time window, we will use the current timestamp to create 'processing_time' (the time we receive the data) and apply window on this timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "binary-kinase",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streaming_review_time = streaming_review.withColumn('processing_time', fn.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5854d85e-ae61-408a-9d97-50449d0208ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "streaming_sentiment = svm_model.transform(streaming_review_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1649ffa4-8a01-4e91-814c-a320be3c581b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streaming_sentiment = streaming_sentiment.withColumn('predicted', fn.when(fn.col('prediction')==1.0, 'positive').otherwise('negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "stainless-institution",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sentiment_count = streaming_sentiment.groupBy(streaming_sentiment.predicted).count()\n",
    "sentiment_window_count = streaming_sentiment.groupBy(fn.window('processing_time','60 seconds'),streaming_sentiment.predicted).count()\n",
    "sentiment_sliding_window_count = streaming_sentiment.groupBy(fn.window('processing_time','60 seconds','30 seconds'),streaming_sentiment.predicted).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a61f18c6-f6be-4295-b746-405c29a21b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_count.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "20051cb2-0c33-404b-adfc-1a3d4fd0a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[predicted: string, count: bigint]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "marked-religion",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_count.isStreaming)\n",
    "print(sentiment_window_count.isStreaming)\n",
    "print(sentiment_sliding_window_count.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aging-proxy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_sentiment = (\n",
    "    sentiment_count\n",
    "    .writeStream\n",
    "    .format('memory')\n",
    "    .queryName('sentiment')\n",
    "    .outputMode('complete')\n",
    "    .start()\n",
    ")\n",
    "query_sentiment_window = (\n",
    "    sentiment_window_count\n",
    "    .writeStream\n",
    "    .format('memory')\n",
    "    .queryName('sentiment_window')\n",
    "    .outputMode('complete')\n",
    "    .start()\n",
    ")\n",
    "query_sentiment_sliding_window = (\n",
    "    sentiment_sliding_window_count\n",
    "    .writeStream\n",
    "    .format('memory')\n",
    "    .queryName('sentiment_sliding_window')\n",
    "    .outputMode('complete')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "df2323bc-b71d-495c-bbf0-84aa51f7b281",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+------+---------+-----+\n",
      "|window|predicted|count|\n",
      "+------+---------+-----+\n",
      "+------+---------+-----+\n",
      "\n",
      "+------+---------+-----+\n",
      "|window|predicted|count|\n",
      "+------+---------+-----+\n",
      "+------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|   63|\n",
      "| negative|   55|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |55   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |63   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |45   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |39   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  126|\n",
      "| negative|  106|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |78   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |68   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |123  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |141  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |123  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |107  |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  235|\n",
      "| negative|  212|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |190  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |173  |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |163  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |192  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |235  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |212  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|positive |61   |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|negative |65   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  339|\n",
      "| negative|  303|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |248  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |276  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |43   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |25   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |163  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |192  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |321  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |287  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|positive |190  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|negative |165  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |43   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |25   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  413|\n",
      "| negative|  359|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |248  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |276  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |74   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |56   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |192  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |163  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |287  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |321  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|negative |188  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|positive |211  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |64   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |48   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  413|\n",
      "| negative|  359|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |276  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |248  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |74   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |56   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |192  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |163  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |287  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |321  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|negative |196  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|positive |221  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |56   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |74   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  445|\n",
      "| negative|  384|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |276  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |248  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |56   |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |74   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |192  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |163  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |287  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |321  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|positive |221  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|negative |196  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |106  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |81   |\n",
      "|{2023-03-19 18:43:30, 2023-03-19 18:44:30}|negative |25   |\n",
      "|{2023-03-19 18:43:30, 2023-03-19 18:44:30}|positive |32   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive|  458|\n",
      "| negative|  408|\n",
      "+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |63   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |55   |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |276  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |248  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |119  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |105  |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|positive |18   |\n",
      "|{2023-03-19 18:41:00, 2023-03-19 18:42:00}|negative |16   |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|negative |163  |\n",
      "|{2023-03-19 18:41:30, 2023-03-19 18:42:30}|positive |192  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|positive |321  |\n",
      "|{2023-03-19 18:42:00, 2023-03-19 18:43:00}|negative |287  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|positive |221  |\n",
      "|{2023-03-19 18:42:30, 2023-03-19 18:43:30}|negative |196  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|negative |105  |\n",
      "|{2023-03-19 18:43:00, 2023-03-19 18:44:00}|positive |119  |\n",
      "|{2023-03-19 18:43:30, 2023-03-19 18:44:30}|negative |54   |\n",
      "|{2023-03-19 18:43:30, 2023-03-19 18:44:30}|positive |47   |\n",
      "|{2023-03-19 18:44:00, 2023-03-19 18:45:00}|positive |2    |\n",
      "|{2023-03-19 18:44:00, 2023-03-19 18:45:00}|negative |5    |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "current_time = time.time()\n",
    "while current_time - start_time <= 120:\n",
    "    spark.sql('select * from sentiment').show()\n",
    "    spark.sql('select * from sentiment_window order by window').show(truncate=False)\n",
    "    spark.sql('select * from sentiment_sliding_window order by window').show(truncate=False)\n",
    "    current_time = time.time()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "parallel-daisy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_sentiment.stop()\n",
    "query_sentiment_window.stop()\n",
    "query_sentiment_sliding_window.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8762b4f-e9e1-41b3-baa1-7bb0196554ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
